\input{../pre.tex}
%\fontfamily{qcr}\selectfont 
%\usepackage[backend=bibtex]{biblatex}
\usepackage[
backend=biber,
style=alphabetic,%firstinits,
citestyle=ieee-alphabetic,
%natbib=true,
%uniquelist=false,
maxnames=10,
sorting=ynt
]{biblatex}
%\addbibresource{writeup/article/refs.bib}
%\title{\vspace{-1cm}}
\title{\textbf{ADVANCED ALGORITHM DESIGN}\\ Homework $2$}
\usepackage{quiver}
\usepackage[nobottomtitles*]{titlesec}
\usepackage{titletoc}
\titleformat{\section}[runin]
  {\normalfont\Large\bfseries}
  {}{0pt}{}%
  [\ifthenelse{\equal{\thesection}{0}}{\\\vspace*{0pt}}{\space\thesection}]
%\author{{\Large NILAVA METYA} \\ 
%\href{mailto:nilava.metya@rutgers.edu}{nilava.metya@rutgers.edu}\\
%\href{mailto:nm8188@princeton.edu}{nm8188@princeton.edu}}

\date{\vspace{-0.7in}October $27$, $2024$}
\newcommand{\pb}{\section{Problem}~\par}
\newcommand{\soln}{\subsection*{Solution}}
\usepackage{pdfpages}
\usepackage{fancyhdr}
	\pagestyle{fancyplain}
	\fancyhf{}
	\fancyhead[R]{\thepage}
\newcommand{\fa}{~\forall~}
\begin{document}

\maketitle


\pb
Recall the max-flow problem from undergraduate algorithms: for a directed graph $G(V,E)$ with non-negative capacities $c_{e}$ for every $e \in E$ and two special vertices $s$ (source, with no incoming edges) and $t$ (sink, with no outgoing edges), a flow in $G$ is an assignment $f : E \to R_{\ge0}$ such that $f(e)\le c_{e}$ for every edge $e$, and for every vertex $v \in V \smallsetminus\set{s,t}$, $\ds\sum_{(u,v)\in E} f((u,v)) = \sum_{(v,w)\in E}f((v,w))$. The task is to find a maximum flow $f$, that is, a flow such that $\ds\sum_{(s,u)\in E}f((s,u))$ is maximized.
\begin{enumerate}[label=(\alph*)]
\item Show that the following LP is a valid formulation for computing the value of the maximum flow in $G$. There is a variable $f((u,v))$ for all $(u,v) \in E$.
\begin{equation}\label{maxflow}\begin{aligned}
\max_{(f((i,j)))\in \R^{E}}& \sum_{u}f((u,t))\\
\text{s.t. }& f((u,v)) \le c_{(u,v)}\fa (u,v)\in E\\
& \sum_{(u,v)\in E} f((u,v)) = \sum_{(v,w)\in E}f((v,w))\fa v\in V\smallsetminus\set{s,t}\\
& f(e)\ge 0\fa e\in E.
\end{aligned}\end{equation}
\item Write the dual for the LP \ref{maxflow}. Show that this dual LP computes the minimum fractional $s-t$ cut in $G$. A fractional cut places each node $v$ at some point $y_{v}$ on the unit interval $[0,1]$, with $s$ placed at $0$ and $t$ placed at $1$. The value of the fractional cut is $\sum\limits_{e=(u,v)\in E(G)} c_{e}\cdot \max\set{0,y_{v}-y_{u}}$ (where $c_{e}$ is the weight of edge $e$). Observe that if instead each $y_{v} \in {0,1}$, that this is simply an $s-t$ cut. Use strong LP duality to conclude the fractional max-flow min-cut theorem. That is, if the max-flow is $C$, there exists a fractional $s-t$ cut of value $C$, and no fractional $s-t$ cut of value $<C$.
\item Devise a rounding scheme that takes as input a fractional min-cut of value $C$ and outputs a true (deterministic) min-cut of value $C$. (Hint: try correlated randomized rounding — choose a threshold $c \in [0,1]$ uniformly at random and set $S = \set{v\st y_{v} \le c}$. What can you say about the probability that a given edge is in this (randomized) cut?) Conclude the max-flow min-cut theorem.
\end{enumerate}


\soln

\begin{enumerate}[label=(\alph*)]
\item For a valid flow $f$ on $G$ (that is, $f$ is feasible to \ref{maxflow}), we denote by $T(v) = T_{f}(v)$ to be the total flow \textit{through} a vertex $v\in V$. More concretely, $\ds T(v) = \sum_{i\to v} f((i,v)) - \sum_{v\to j} f((v,j))$. So $T(s) = - \sum\limits_{s\to j} f((s,j))$ and $T(t) = \sum\limits_{i\to t} f((i,t))$. $f$ is constrained to be conserved at each vertex $v\in V\smallsetminus\set{s,t}$, so $T(v) = 0\fa v\in V\smallsetminus\set{s,t}$. This means $\sum_{v\in V}T(v) = T(s) + T(t)$. However, every outgoing edge $e$ for some vertex is an incoming edge for some other vertex. That is to say, every edge in the above summation occurs positively and negatively exactly once each. So $\sum_{v\in V}T(v)=0$. This establishes $\sum_{v}f((s,v)) = \sum_{u}f((u,t))$ for any valid flow $f$. This justifies the objective of \ref{maxflow}. The constraints simply come from the problem description, namely respectively, that the flow value in each edge is at most the capacity, the conservation law holds at every vertex except $s,t$, and the flow is non-negative on every edge.
\item The dual problem will have a dual variable per constraint and will be formed as a minimization problem. Let the dual variables be $x_{(i,j)}$ for each $(i,j)\in E$, and $y_{i}$ for each $i\in V\smallsetminus\set{s,t}$. The objective is to minimize $\sum_{e\in E}c_{e}x_{e}$ with the constraints that $x_{e}\ge 0\fa e\in E, 1 \le x_{(u,t)} - (-y_{u})\fa (u,t)\in E$ where $u\ne s$, $0\le x_{(s,u)} - y_{u} \fa (s,u)\in E$ where $u\ne t$, $1\le x_{(s,t)}$ if $(s,t)\in E$, $y_{v}-y_{u}\le x_{(u,v)}\fa (u,v)\in E, v\ne t, u\ne s$.

We justify this as follows. Firstly it is not hard to see that (using the same analysis as done in class) that the following is a primal dual pair (without any duality gap):
\begin{align*}
\max_{\pmb f}~& \pmb a^{\top}\pmb f & \min_{\pmb x,\pmb y}~ & \pmb c^{\top}\pmb x\\
\text{s.t. }& A\pmb f=0 & \text{s.t. } & \pmb c \le \pmb x - A^{\top}\pmb y\\
& \pmb f\ge 0 & & \pmb x\ge 0\\
& \pmb f\le \pmb c\\
\end{align*}
The $A$ matrix has columns indexed by $E$ and rows indexed by $V\smallsetminus\set{s,t}$, with entries $0,\pm 1$. To explain the entries of $A$, we just illustrate it for one row, namely the vertex given by $v$ - there is a $1$ at column $(u,v)$ for each such valid edge, $-1$ at column $(v,w)$ for each such valid edge, and $0$ elsewhere. So $A\pmb f=0$ forces the conservation-of-flow constraint. The other two constraints, namely $\pmb c\ge \pmb f\ge 0$ are self explanatory. 

The dual is obtained by pattern matching with the above. There are dual variables $x_{e}$ for each edge capacity constraint and $y_{v}$ ($v\ne s,t$) for each vertex flow-conservation constraint. The vector $\pmb a$ has entry $1$ exactly for the edges going to $t$, and $0$ elsewhere. So the corresponding dual constraints have a scalar $1$ on one side of the inequality for edges going into $t$, and $0$ for others. Now for each column $(u,v)$ (edge-labeled) of $A$ we have to satisfy the constraint that looks like $c_{(u,v)}\le x_{(u,v)} - y_{v}+y_{u}$. It looks a little different if $u=s$ or $v=t$ and changes into the aforementioned constraints. Combining all of these, the required dual program is 

\begin{equation}\label{mincut}\begin{aligned}
\min_{(x_{(i,j)})\in \R^{E}, (y_{v})\in \R^{V\smallsetminus\set{s,t}}}& \sum_{(i,j)\in E}c_{(i,j)}x_{(i,j)}\\
\text{s.t. }& x_{e}\ge 0\fa e\in E\\
& 1 \le x_{(u,t)} + y_{u} \fa (u,t)\in E,u\ne s\\ 
& 0\le x_{(s,u)} - y_{u} \fa (s,u)\in E, u\ne t\\ 
& 1\le x_{(s,t)} \text{ if } (s,t)\in E\\ 
& y_{v}-y_{u}\le x_{(u,v)}\fa (u,v)\in E, v\ne t, u\ne s.
\end{aligned}\end{equation}

The last four constraints can be written in one line as $x_{(u,v)}+y_{u}-y_{v}\ge 0\fa (u,v)\in E$ with the understanding that $y_{s}=0,y_{t}=1$. This combined with the non-negativity of $\pmb x$ is same as saying $x_{e=(u,v)}\ge \max\set{0,y_{v}-y_{u}}\fa e\in E$. So our modified program is $\ds\min_{\pmb x,\pmb y} \sum_{e\in E}c_{e}x_{e}$ subject to $\ds x_{e=(u,v)}\ge \max\set{0,y_{v}-y_{u}}\fa e\in E$. Since $c\ge 0$, the minimum value occurs when each $x_{e}$ is minimum, so the $\pmb x$ can be eliminated to get an equivalent LP that $\ds\min_{\pmb y}\sum\limits_{e=(u,v)\in E}c_{e}\max\set{0,y_{v}-y_{u}}$ subject to $y_{s}=0,y_{t}=1$. Now note that if any $(u,t)$ is such that $y_{u}>1$, then forcefully setting $f_{u}=1$ does not change the objective value. Extend this to all vertices one by one to conclude that there is an optimal solution with $y_{u}\le 1\fa u\in V$. Same argument, starting with (forward) neighbors of $s$ will give that there is an optimal solution with $y_{u}\ge 0\fa u\in V$. Thus our equivalent LP is \begin{equation}\label{mincut}\begin{aligned}
\min_{\pmb y\in \R^{V}} &\sum\limits_{(u,v)\in E}c_{(u,v)}\max\set{0,y_{v}-y_{u}}\\
\text{s.t. }& 1\ge y_{u}\ge 0\fa u\in V\\
& y_{s}=0, y_{t}=1.
\end{aligned}\end{equation}
This was exactly what was asked, to minimize the given cut value by assigning each vertex a real number in $[0,1]$.
\item We randomly choose a $c\in (0,1)$ and set $S\sett \set{v\in V\st y_{v}\le c}$ and $T\sett \overline S = V\smallsetminus S$. This gives an $s-t$ cut $(S_{c},T_{c})$. Say $(y_{v})_{v\in V}$ determines a fractional mincut of value $C = \sum_{(u,v)\in E}c_{(u,v)}\max\set{0,y_{v}-y_{u}}$. We round it using our randomized rounding scheme. Then its cut value is $\chi_{c}=\sum\limits_{e\in E}\pmb 1[e \text{ is cut}]\cdot c_{e}$. Note that $\chi_{c}\ge C$ for any $c\in (0,1)$ because any rounding determined by $c$ gives a valid cut which must be at least the optimal, namely $C$. In expectation $\ds\E{\chi_{c}} = \sum\limits_{e\in E}c_{e}\cdot \P{e \text{ is cut}}$. However $\P{(u,v) \text{ is cut}} = \P{y_{u}\le c < y_{v}} \le y_{v}-y_{u}$. But probabilities are $\ge 0$, so actually $\P{(u,v) \text{ is cut}} \le \max\set{0,y_{v}-y_{u}}$. This means $\E{\chi_{c}} \le \sum\limits_{(u,v)\in E}c_{(u,v)}\max\set{0,y_{v}-y_{u}} = C$. We have established that the expectation of the non-negative random variable $\chi_{c}-C$ is $0$, which means $\chi_{c}-C\equiv 0$. In other words, $\chi_{c}=C\fa c\in (0,1)$. 

Therefore our rounding procedure is to take $c=\frac12$ and  $S=\set{v\in V\st y_{v}\le\frac12},T=V\smallsetminus S$.


%Since the average of $\chi_{c}$ (taken over $c\in(0,1)$) is at most $C$, it must happen that $\exists \tilde c\in (0,1)$ such that our rounding procedure gives a cut of value $\chi_{\tilde c}\le C$. However the corresponding cut $(S_{\tilde c},T_{\tilde c})$ also determines a `fractional' cut $(\tilde y_{v})_{v\in V}$, just with values in $\set{0,1}$ and each summand in the objective of the LP \ref{mincut} only contributes when $y_{v}-y_{u}=1$, that is $(u,v)$ is a cut vertex. Since $C$ is the minimal value attainable by a fractional cut, it must happen that $\chi_{\tilde c} \ge C$. Combining them gives $\chi_{\tilde c} = C$. 
\end{enumerate}







\newpage
\pb

The maximum cut problem asks us to cluster the nodes of a graph $G = (V,E)$ into
two disjoint sets $X,Y$ so as to maximize the number of edges between these sets:
$$\max_{X,Y} \sum_{(i,j)\in E}\pmb 1\left[(i\in X, j\notin X) \lor (i\in Y, j\notin Y)\right].$$
Consider instead clustering the nodes into three disjoint sets $X,Y,Z$. Our goal is to maximize the number of edges between different sets:
$$\max_{X,Y,Z} \sum_{(i,j)\in E}\pmb 1\left[(i\in X, j\notin X) \lor (i\in Y, j\notin Y)(i\in Z, j\notin Z)\right].$$

Design an algorithm based on SDP relaxation that solves this problem with approximation ration greater then $0.7$.

\soln

(We assume undirected graph $G$ just to write the notation $\set{i,j}$)
For the problem with two partitions, we had modeled the problem with having variables $x_{v}\in \set{\pm 1}$ for each vertex $v\in V$. For the corresponding problem with three partitions we will restrict each such variable to be a $2-$vector among $\pmb a_{1} \sett (1,0), \pmb a_{2} \sett\left(-\frac12,\frac{\sqrt3}{2}\right), \pmb a_{3} \sett \left(-\frac12,-\frac{\sqrt3}{2}\right)$. It is easy to verify that $\pmb a_{1}^{\top}\pmb a_{2} = \pmb a_{2}^{\top}\pmb a_{3} = \pmb a_{3}^{\top}\pmb a_{1} = -\frac12$. The three vertices $\pmb a_{1,2,3}$ stand for the three partitions $X,Y,Z$. Any edge $(u,v)\in E$ that gets assigned different classes of vertices, say $\pmb x_{u} = \pmb a_{1}, \pmb x_{v} = \pmb a_{2}$, contributes exactly $1 = \frac{2}{3}\left(1-\pmb a_{1}^{\top}\pmb a_{2}\right)$ to the cut value. If they are in the same class then $\pmb x_{u} = \pmb x_{v}$ and $\pmb x_{u}^{\top}\pmb x_{v} = 1$ giving a contribution of $0$ from the expression $\frac{2}{3}\left(1-\pmb x_{u}^{\top}\pmb x_{v}\right)$.

Let's make things formal now. Let $G=(V=[n],E)$ be the given graph. Introduce variables $\pmb x_{v}\in \R^{2}$, one for each $v\in V$, and constrain them $\pmb x_{v}\in \set{\pmb a_{1},\pmb a_{2}, \pmb a_{3}}$ where $\pmb a_{i}$ are as in the above paragraph. Given the above discussion, our problem is modeled as follows
\begin{equation}\label{max3cut}\begin{aligned}
f^{*}\sett \max_{\pmb x_{1},\cdots, \pmb x_{n}\in \R^{2}} &~~~~\frac{2}{3} \sum_{(i,j)\in E} (1-\pmb x_{i}^{\top}\pmb x_{j})\\
\text{s.t. } &~~~~ \pmb x_{i}\in \set{\pmb a_{1},\pmb a_{2}, \pmb a_{3}} \fa i\in V
\end{aligned}\end{equation}

We are essentially interested in $\ds\min_{\pmb x_{1},\cdots, \pmb x_{n}\in \R^{2}}\frac{2}{3} \sum_{(i,j)\in E} \pmb x_{i}^{\top}\pmb x_{j}$ s.t. $\pmb x_{i}\in \set{\pmb a_{1},\pmb a_{2}, \pmb a_{3}} \fa i\in V$.

To get an SDP \underline{relaxation}, we relax our constraints to $\norm{\pmb x_{i}}{2} = 1\fa i\in V$ and $\pmb x_{i}^{\top}\pmb x_{j} \ge -\frac{1}{2}$. The last constraint gives the best angle separation among $3$ vectors on $\mathbb S^{1}$ in the following sense: if $t\in \R$ is such that $\pmb v_{1},\pmb v_{2},\pmb v_{3}\in \mathbb S^{14}$ satisfy $\pmb v_{i}^{\top}\pmb v_{j} \le t ~(\forall~ i\ne j)$ then $0\le \norm{\pmb v_{1}+\pmb v_{2}+\pmb v_{3}}{2}^{2} = 3 + 2\cdot 3 \cdot t \implies t\ge -1/2$. So we design an SDP with the rank$-2$ matrix $\begin{bmatrix}\pmb x_{1}^{\top} \\ \vdots \\ \pmb x_{n}^{\top}\end{bmatrix}_{n\times 2}\begin{bmatrix}\pmb x_{1} & \cdots & \pmb x_{n}\end{bmatrix}_{2 \times n} \succeq 0$ in mind:

\begin{equation}\label{sdprelax}\begin{aligned}
\frac{2m}{3}- f_{S} = \min_{X\in S^{n\times n}} &~~~~  \Tr\left[\frac{2}{3}QX\right]\\
\text{s.t. } &~~~~ X_{ii} = 1 \fa i\in V\\
&~~~~ X_{ij} \ge -\frac{1}{2} \fa i\ne j\in V\\
&~~~~ X \succeq 0
\end{aligned}\end{equation}
where $Q$ is a matrix whose $(i,j)^{\text{th}}$ entry is $\frac{1}{2}$ if $\set{i,j}\in E$ and $0$ otherwise, $S^{n\times n}$ denotes the space of all real symmetric $n\times n$ matrices, and $f_{S}$ is the optimal value obtained from SDP relaxation. 

Let's say the this SDP attains its optimal solution at $X^{*}$. Take a Cholesky factorization $X^{*} = V^{\top}V$ where $V\in \R^{r\times n}$ and $r=\rk V$. Say $V$ has columns $\pmb y_{1},\cdots, \pmb y_{n}\in \mathbb S^{r-1}$ (unit vectors because diagonal of $X$ is $1$). So $\frac{2m}3 - f_{S} = \frac{2}{3} \sum \limits_{\set{i,j}\in E} \pmb y_{i}^{\top}\pmb y_{j}\implies f_{S} = \frac{2}{3}\sum \limits_{\set{i,j}\in E} \left(1-\pmb y_{i}^{\top}\pmb y_{j}\right) = \frac23\sum\limits_{\set{i,j}\in E}(1-\cos\theta_{ij})$ where $\theta_{ij}\in [0,2\pi)$ is the angle between $\pmb y_{i},\pmb y_{j}$. We relaxed a maximization problem so $f^{*}\le f_{S}$. Also note that $\cos\theta_{ij}=X_{ij}\ge -\frac12$ so $\theta_{ij} \in \left[0, \frac{2\pi}{3}\right]\cup\left[\frac{4\pi}3,2\pi\right]$. 

%In the rounding step, choose random vectors $\pmb R_{1}, \pmb R_{2}, \pmb R_{3}\in \R^{r}$ such that each $R_{i,j}\sim \mathcal N(0,1)$ (for $1\le j \le r$) is chosen independently. These will give us the partitions, by rounding each $\pmb y_{i}$ to the component nearest among $\pmb R_{j}$. More precisely, we partition $V = V_{1}\sqcup V_{2}\sqcup V_{3}$ as follows:
%\begin{align*}
%V_{1} &\sett \set{i\in V \st \pmb y_{i}^{\top} \pmb R_{1} \ge \pmb y_{i}^{\top} \pmb R_{2},\pmb  y_{i}^{\top} \pmb R_{1} \ge \pmb y_{i}^{\top} \pmb R_{3}}\\
%V_{2} &\sett \set{i\in V \st \pmb y_{i}^{\top} \pmb R_{2} \ge \pmb y_{i}^{\top} \pmb R_{1},\pmb  y_{i}^{\top} \pmb R_{2} \ge \pmb y_{i}^{\top}\pmb  R_{3}}\\
%V_{3} &\sett \set{i\in V \st \pmb y_{i}^{\top} \pmb R_{3} \ge \pmb y_{i}^{\top} \pmb R_{2},\pmb y_{i}^{\top} \pmb R_{3} \ge \pmb y_{i}^{\top}\pmb  R_{1}}
%\end{align*}
%while breaking ties at random (such a thing happens with $0$ probability). In fact assign $\pmb x_{i} \sett \pmb a_{j}$ if $i\in V_{j}$.
%

%
%{\color{red}need to analyze bound}
%
%Let's look at two vertices, say $1,2$, and try to find $\P{\pmb x_{1}\ne \pmb x_{2}}$. This is same as the probability that $1,2$ are separated by $\pmb R_{1},\pmb R_{2},\pmb R_{3}$. Now, 
%\begin{align*}
%\mathbb P[1,2 \text{ not } & \text{separated by } \pmb R_{1},\pmb R_{2},\pmb R_{3}] = 3\cdot \P{\pmb y_{1}^{\top}\pmb R_{1} = \max_{i=1,2,3} \pmb y_{1}^{\top}\pmb R_{i} \text{ and } \pmb y_{2}^{\top}\pmb R_{1} = \max_{i=1,2,3} \pmb y_{2}^{\top}\pmb R_{i}}\\
%&= 3\iint \P{\pmb y_{1}^{\top}\pmb R_{1} = \max_{i} \pmb y_{1}^{\top}\pmb R_{i}, \pmb y_{2}^{\top}\pmb R_{1} = \max_{i} \pmb y_{2}^{\top}\pmb R_{i} \st \pmb y_{1}^{\top}\pmb R_{1} = \alpha, \pmb y_{2}^{\top}\pmb R_{1} = \beta}\cdot \\&~~~~~~~~~~~~~~~~~~\P{\pmb y_{1}^{\top}\pmb R_{1} = \alpha, \pmb y_{2}^{\top}\pmb R_{1} = \beta}\d \alpha~\d \beta\\
%&= 3\iint \P{\alpha \ge \pmb y_{1}^{\top}\pmb R_{i=2,3}, \beta \ge \pmb y_{2}^{\top}\pmb R_{i=2,3}}\P{\pmb y_{1}^{\top}\pmb R_{1} = \alpha, \pmb y_{2}^{\top}\pmb R_{1} = \beta}\d \alpha ~\d \beta\\
%&=3\iint \P{\alpha \ge \pmb y_{1}^{\top}\pmb R_{2}, \beta \ge \pmb y_{2}^{\top}\pmb R_{2}} \cdot \P{\alpha \ge \pmb y_{1}^{\top}\pmb R_{3}, \beta \ge \pmb y_{2}^{\top}\pmb R_{3}}\cdot\\ &~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\P{\pmb y_{1}^{\top}\pmb R_{1} = \alpha, \pmb y_{2}^{\top}\pmb R_{1} = \beta}\d \alpha ~\d \beta\\
%&= 3\iint \P{\alpha \ge \pmb y_{1}^{\top}\pmb R_{1}, \beta \ge \pmb y_{2}^{\top}\pmb R_{1}}^{2} \P{\pmb y_{1}^{\top}\pmb R_{1} = \alpha, \pmb y_{2}^{\top}\pmb R_{1} = \beta} \d \alpha ~\d \beta
%\end{align*}
%where the second-last equality is due to the independence of $\pmb R_{2},\pmb R_{3}$ and the last line is because $\pmb R_{1},\pmb R_{2}, \pmb R_{3}$ are identical. From now, we will say $\pmb R$ to mean an identical independent copy of $\pmb R_{1}$.
%
%To compute the above, we can assume that $\pmb y_{1}=(1,0,\cdots,0), \pmb y_{2} = (\cos\theta,\sin\theta,0,\cdots,0)$ because $\pmb R_{i}$ are spherically symmetric and can only detect the angle between $\pmb y_{1},\pmb y_{2}$. Therefore we want to compute $F(\alpha,\beta,\theta) = \P{\alpha \ge \pmb y_{1}^{\top}\pmb R, \beta \ge \pmb y_{2}^{\top}\pmb R} = \P{\alpha \ge g_{1}, \beta \ge g_{1}\cos\theta+g_{2}\sin\theta}$, where the first two entries of $\pmb R$ are independent $g_{1},g_{2}\sim \cN(0,1)$, and its corresponding density $f(\alpha,\beta,\theta)$ will determine $\P{\pmb y_{1}^{\top}\pmb R = \alpha, \pmb y_{2}^{\top}\pmb R = \beta}$. Now, $F(\alpha,\beta,\theta) = \frac{1}{2\pi}\int\limits_{-\infty}^{\alpha}\int\limits_{-\infty}^{\frac{\beta-x_{1}\cos\theta}{\sin\theta}} \exp\set{\frac{-(x_{1}^{2}+x_{2}^{2})}{2}} \d x_{2}~\d x_{1}$.
%In order to change the inner integral to $\int_{-\infty}^{\beta}$ (so that we can find $f$) we introduce a variable $z_{2}$ such that $x_{2}=\frac{z_{2}-x_{1}\cos\theta}{\sin \theta}$ so that $z_{2}\in(-\infty,\beta]$ (and take $z_{1}=x_{1}$). This makes the integral
%\begin{align*}
%F(\alpha,\beta,\theta) &= \frac{1}{2\pi\sin\theta}\int\limits_{-\infty}^{\alpha}\int\limits_{-\infty}^{\beta} \exp\set{\frac{-(z_{1}^{2}+z_{2}^{2}-2z_{1}z_{2}\cos\theta)}{2\sin^{2}\theta}} \d z_{2}~\d z_{1}
%\end{align*}
%This gives $f(z_{1},z_{2},\theta) = \frac{1}{2\pi\sin\theta}\exp\set{\frac{-(z_{1}^{2}+z_{2}^{2}-2z_{1}z_{2}\cos\theta)}{2\sin^{2}\theta}} \le \frac{1}{2\pi\sin\theta}\exp\set{\frac{-(z_{1}+z_{2})^{2})}{2}}$.

%In the rounding step, we choose a random vector $\pmb R \in \mathbb S^{r-1}$ uniformly and take inner products $s_{i} = \inner{\pmb y_{i}}{\pmb R}$. Then we devise the following scheme: if $s_{i}\in [-1,-1/2],$ take $\pmb x_{i}=\pmb a_{1}$, if $s_{i}\in (-1/2,1/2],$ take $\pmb x_{i}=\pmb a_{2}$, otherwise $s_{i}\in (1/2,1]$ and take $\pmb x_{i}=\pmb a_{3}$.
In the \underline{rounding} step, we choose two random vector $\pmb R, \pmb W \in \mathbb S^{r-1}$ uniformly and take inner products $s_{i} = \inner{\pmb y_{i}}{\pmb R}, t_{i} = \inner{\pmb y_{i}}{\pmb W}$. Then we devise the following scheme: if $s_{i} > 0$ take $\pmb x_{i}=\pmb a_{1}$, if $s_{i} \le 0, t_{i}>0,$ take $\pmb x_{i}=\pmb a_{2}$, otherwise $s_{i}\le 0, t_{i}\le 0$ and take $\pmb x_{i}=\pmb a_{3}$.

Let $f_{R}$ denote the cut value produced by the above-mentioned randomized rounding. So $f_{R} = \sum\limits_{\set{i,j}\in E} \pmb 1\left[\pmb x_{i}\ne \pmb x_{j}\right]$. We are interested in $\ds \E{f_{R}} = \sum\limits_{\set{i,j}\in E} \P{\pmb x_{i}\ne \pmb x_{j}}$ because we eventually want to bound $\frac{\E{f_{R}}}{f^{*}}$ which is already $\ge \frac{\E{f_{R}}}{f_{S}} = \frac{\sum\limits_{\set{i,j}\in E}\P{\pmb x_{i}\ne \pmb x_{j}}}{\frac23 \sum\limits_{\set{i,j}\in E} (1-\cos\theta_{ij})}$. 

%Let's just focus on one quantity $\P{\pmb x_{1}\ne \pmb x_{2}} = 1-\P{\pmb x_{1}=\pmb x_{2}} = 1 - \P{\pmb x_{1}=\pmb x_{2} = \pmb a_{1}}$ because $\P{\pmb x_{1}=\pmb a_{1}} = \P{\pmb x_{1} = \pmb a_{2}} = \P{\pmb x_{1} = \pmb a_{3}}$. But $\P{\pmb x_{1}=\pmb x_{2} = \pmb a_{1}}$ depends only on the angle $\theta=\theta_{1,2}$ and since $\pmb R$ is spherically symmetric and it depends only on its projection onto the intersection of $\mathbb S^{r-1}$ and the hyperplane spanned by $\pmb x_{1},\pmb x_{2}$ (which is uniform on the circle), we can WLOG assume $\pmb x_{1}=(1,0), \pmb x_{2} = (\cos\theta,\sin\theta)$ and that $\pmb R=(\cos\alpha,\sin\alpha)$ where $\alpha\sim\text{Unif}([0,2\pi))$. But $\pmb x_{1}=\pmb a_{1}\iff \cos\alpha\in [-1,-1/2]\iff \alpha\in \left[\frac{2\pi}{3},\frac{4\pi}{3}\right]$ and $\pmb x_{2}=\pmb a_{1}\iff \cos(\theta-\alpha) = \cos\alpha\cos\theta + \sin\alpha\sin\theta \in [-1,-1/2]$. If $\theta \le \frac{2\pi}{3}$ then the feasible values of $\alpha$ are $\left[\frac{2\pi}{3},\frac{4\pi}{3}\right] \cap \left[\frac{2\pi}{3}+\theta,\frac{4\pi}{3}+\theta\right] = \left[\frac{2\pi}{3}+\theta,\frac{4\pi}{3}\right]$. If $\theta \ge \frac{4\pi}{3}$ then the feasible values of $\alpha$ are $\left[\frac{2\pi}{3},\frac{4\pi}{3}\right] \cap \left[\frac{2\pi}{3}-\phi,\frac{4\pi}{3}-\phi\right] = \left[\frac{2\pi}{3},\frac{4\pi}{3}-\phi\right]$ where $\phi\sett 2\pi-\theta\in \left[0,\frac{2\pi}{3}\right]$. In the former case, the length of the feasible region of $\alpha$ is $\frac{2\pi}{3}-\theta = \abs{\pi-\theta} - \frac{\pi}{3}$, and the same for the latter case is $\frac{2\pi}{3}-\phi = \theta-\frac{4\pi}{3} = \abs{\theta-\pi} - \frac{\pi}{3}$. Both these lengths are equal. Therefore $\P{\pmb x_{1}=\pmb x_{2}=\pmb a_{1}} = \frac{\abs{\theta-\pi} - \frac{\pi}{3}}{2\pi} = \abs{\frac{\theta}{2\pi}-\frac12} - \frac16\implies \P{\pmb x_{1}\ne \pmb x_{2}} = \frac76 - \abs{\frac{\theta}{2\pi}-\frac12}$. WolframAlpha gives that $\frac76 - \abs{\frac{\theta}{2\pi}-\frac12} \ge \frac{2}{3} (1-\cos \theta)$ for $\theta\in [0,2\pi/3]\cup[4\pi/3,2\pi]$. Therefore our approximation ratio is $\ge \frac{2}{3}\times \frac32 = 1$.

Now let's \underline{bound}. Focus on one quantity $\P{\pmb x_{1}\ne \pmb x_{2}} = 1-\P{\pmb x_{1}=\pmb x_{2}} = 1 - \P{\pmb x_{1}=\pmb a_{1},\pmb x_{2} = \pmb a_{1}} - \P{\pmb x_{1}=\pmb a_{2},\pmb x_{2} = \pmb a_{2}}-\P{\pmb x_{1}=\pmb a_{3},\pmb x_{2} = \pmb a_{3}}$. We are only using inner products, so the spherical symmetry of $\pmb R,\pmb W$ implies that it is enough to take them as uniformly random points on the intersection of $\mathbb S^{r-1}$ and the hyperplane spanned by $\pmb x_{1},\pmb x_{2}$ (because it's essentially the projection). This intersection looks like $\mathbb S^{1}$. Therefore think of $\pmb R,\pmb W$ as independent $\alpha,\beta \sim \text{Unif}((-\pi,\pi])$ where we think of all angles modulo $2\pi$. So $-\frac\pi2$ would mean $\frac{3\pi}{2}$. We can also WLOG take $\pmb x_{1}=(1,0),\pmb x_{2}=(\cos\theta,\sin\theta)$ with $\theta\in (-\pi,\pi]$. Now the feasible $\alpha,\beta$ for $\P{\pmb x_{1}=\pmb a_{1},\pmb x_{2} = \pmb a_{1}}$ are $\alpha\in \left[\frac{-\pi}{2},\frac{\pi}{2}\right] \cap \left[\frac{-\pi}{2}+\theta,\frac{\pi}{2}+\theta\right] = \begin{cases}\left[\frac{-\pi}{2}+\theta,\frac{\pi}{2}\right] & \text{ if } \theta\ge 0 \\
\left[\frac{-\pi}{2},\frac{\pi}{2}+\theta\right] & \text{ otherwise}
\end{cases}$ and $\beta\in (-\pi,\pi]$. So $\P{\pmb x_{1}=\pmb a_{1},\pmb x_{2} = \pmb a_{1}} = \frac{\pi-\abs \theta}{2\pi}$. Similar analysis shows that $\P{\pmb x_{1}=\pmb a_{2},\pmb x_{2} = \pmb a_{2}} = \P{\pmb x_{1}=\pmb a_{3},\pmb x_{2} = \pmb a_{3}} = \left(\frac{\pi-\abs\theta}{2\pi}\right)^{2}$. Therefore $\P{\pmb x_{1}\ne \pmb x_{2}} = 1 - \frac{\pi-\abs\theta}{2\pi} - \frac{(\pi-\abs\theta)^{2}}{2\pi^{2}}$. WolframAlpha shows that $\ds \frac{1 - \frac{\pi-\abs\theta}{2\pi} - \frac{(\pi-\abs\theta)^{2}}{2\pi^{2}}}{\frac23(1-\cos\theta)} \ge \frac{7}{9} \simeq 0.77$ for $\theta\in \left[0,\frac{2\pi}{3}\right]\cup\left[-\frac{2\pi}{3},0\right]$. We conclude by the previous paragraph that $\E{f_{R}}\ge 0.77 f^{*}$.








\newpage
\pb
The Ellipsoid algorithm we saw in the lecture solves convex programs assuming a separation oracle. Here, we want to show the opposite. To be more specific, consider the following two tasks regarding a convex body $\cK$:
\begin{itemize}
\item $\texttt{OPTIMIZE}(\cK):$ given a vector $\pmb c\in\R^{n}$, output $\arg\max\limits_{\pmb x\in\cK}\pmb c^{\top}\pmb x$;
\item $\texttt{SEPARATE}(\cK):$ given a point $\pmb x\in\R^{n}$, output either $\pmb x\in \cK$ or a separating hyperplane.
\end{itemize}

We are going to show that if for a specific convex body $\cK$, there is a polynomial time algorithm for $\texttt{OPTIMIZE}(\cK)$, then there is a polynomial time algorithm for $\texttt{SEPARATE}(\cK)$.
\begin{enumerate}[label = (\alph*)]
\item Suppose for a given $x$, we can solve the following LP with infinitely many constraints (finding the optimal $w$ and $T$). Show that we can use such an algorithm to solve $\texttt{SEPARATE}(\cK)$.
\begin{equation}\label{margin}
\begin{aligned}
\max_{\pmb w\in \R^{n},T\in\R} &~~ \pmb w^{\top} \pmb x-T\\
\text{s.t.} &~~ \pmb w^{\top} \pmb y \le T\fa \pmb y\in\cK\\
&~~ -1\le T\le 1
\end{aligned}
\end{equation}
\item Design a polytime separation oracle for the above LP using $\texttt{OPTIMIZE}(\cK)$, and conclude.
\end{enumerate}


\soln

\begin{enumerate}[label = (\alph*)]
\item Suppose the value of this LP is $>0$ and is attained at $(\overline {\pmb w},\overline T)$. Then for any $\pmb y\in\cK$, $\overline {\pmb w}^{\top}\pmb y-\overline T \le 0$. This means that $\pmb x\notin \cK$.\\
Suppose $\pmb x\notin \cK$. Then there is a vector $\pmb w\in \R^{n}$ such that $\pmb w^{\top}\pmb x > 0$ and $\pmb w^{\top}\pmb y\le 0\fa \pmb y\in \cK$. This $(\pmb w,T=0)$ is feasible to \ref{margin} with objective $>0$. Thus its optimal value is $>0$.\\
Therefore $\pmb x\in \cK$ iff the optimal value of \ref{margin} is $\le 0$. If $\le 0$ with optimal $\pmb w=\overline {\pmb w}$, a separating hyperplane is $\overline {\pmb w}$ because of what is discussed above.

\item The feasible set of \ref{margin} is $\cS = \set{(\pmb w,T)\in \R^{n+1}\st -1\le T\le 1, \pmb w^{\top}\pmb y\le T\fa \pmb y\in\cK}$. We want to design a polytime separation oracle for $\cS$, that is, given any $(\pmb a,s)\in \R^{n}\times \R$, the oracle will either say $(\pmb a,s)\in \cS$ or will give a separating hyperplane with normal $(\pmb v,t)\in\R^{n}\times \R$ such that $\inner{(\pmb v,t)}{(\pmb w,T)}\le 0\fa (\pmb w,T)\in \cS$ and $\inner{(\pmb v,t)}{(\pmb a,s)}> 0$.\\
Let $(\pmb a,s)$ be an input to the oracle. We have access to $\texttt{OPTIMIZE}(\cK)$, so we can determine $\ds\pmb y^{*}=\arg\max_{\pmb y\in \cK} \pmb a^{\top}\pmb y$. If $\pmb a^{\top}\pmb y^{*} \le s$ then $\pmb a^{\top}\pmb y \le \pmb a^{\top}\pmb y^{*} \le s$ by definition of $\pmb y^{*}$ whence $(\pmb a,s)\in \cS$. Otherwise, $\pmb a^{\top}\pmb y^{*} > s$ and $(\pmb y^{*},-1)$ determines a hyperplane that separates $(\pmb a,s)$ from $\cS$. Indeed if $(\pmb w,T)\in \cS$ then $\inner{(\pmb y^{*},-1)}{(\pmb w,T)} = \pmb w^{\top}\pmb y^{*} - T \le 0$ by definition of $\cS$ and since $\pmb y^{*}\in \cK$, and $\inner{(\pmb y^{*},-1)}{(\pmb a,s)} = \pmb a^{\top}\pmb y^{*} - s > 0$ by our hypothesis on the value $\pmb a^{\top}\pmb y^{*}$. This oracle is polytime because vector inner product operation requires linear (in $n$) many operations and $\texttt{OPTIMIZE}(\cK)$ is a polytime oracle, so the overall number of steps is polynomial in $n$.\\
Once we have a separation oracle for the feasible set of \ref{margin}, we use the ellipsoid algorithm to solve the LP. Since all oracles are polytime and the ellipsoid algorithm is polytime, the LP can be solved in polynomial time. We earlier showed that if the LP can be solved in polynomial time, then we can solve $\texttt{SEPARATE}(\cK)$. This shows how $\texttt{OPTIMIZE}(\cK)$ is used to solve $\texttt{SEPARATE}(\cK)$ in polynomial time.
\end{enumerate}














\newpage
\pb
Describe separation oracles for the following convex sets. Your oracles should run in linear time, assuming that the given oracles run in linear time (so you can make a constant number of black-box calls to the given oracles).
\begin{enumerate}[label=(\alph*)]
\item The $\ell_{1}$ ball, $\set{\pmb x : \norm{\pmb x}{1} \le 1}$. Recall that $\norm {\pmb x}1 = \sum\limits_{i=1}^{n}\abs{\pmb x_{i}}$.
\item Any convex set $A$ that we have a projection oracle for. i.e. we have an oracle to compute $\ds\arg \min_{\pmb x\in A} \norm{\pmb x-\pmb y}{2}$ for any $\pmb y$.
\item The $\epsilon-$neighborhood $E$ of any convex set $A$: $$E=\set{\pmb x\st \exists ~ \pmb y\in A \text{ with } \norm{\pmb x-\pmb y}{2}\le\epsilon}$$ given a projection oracle for $A$.
\end{enumerate}

\soln

\begin{enumerate}[label=(\alph*)]
\item Let the input point be $\pmb a\in \R^{n}$. First we compute $\norm{\pmb a}1 = \sum_{i=1}^{n}\pmb a_{i}$ in $\cO(n)$ time. \\
If $\norm{\pmb a}1 \le 1$, we report that $\pmb a$ is in the $\ell_{1}$ ball. \\
If not, that is $\norm{\pmb a}1 >1$, then the hyperplane $\cH \sett \set{\pmb x\in\R^{n}\st \pmb 1^{\top}\pmb x = \frac{\norm{\pmb a}1+1}{2}}$ where $\pmb 1 = \pmb 1_{n}\in\R^{n}$ is the vector comprising of all one's. Indeed $\pmb 1^{\top} \pmb a = \norm{\pmb a}{1} = \frac{\norm{\pmb a}{1}}{2} + \frac{\norm{\pmb a}{1}}{2} > \frac{\norm{\pmb a}1+1}{2}$, and if $\pmb x$ is in the $\ell_{1}$ ball then $\pmb 1^{\top}\pmb x = \norm{\pmb x}{1} \le \frac{1+1}{2} < \frac{\norm{\pmb a}1+1}{2}$. Again, this hyperplane is computed in $\cO(n)$ steps. Overall it requires at most $2\cO(n) = \cO(n)$ steps.

\item Let's start with a fundamental lemma about projections onto convex sets $A$.
\begin{lemma}\label{obs}
Denote by $\ds\pi(\pmb x) \sett \arg\min_{\pmb y\in A}\norm{\pmb y-\pmb x}{2}$ which is a point in the closure $\overline A$ of $A$. Then $(\pmb x-\pi(\pmb x))^{\top}(\pmb z-\pi(\pmb x))\le 0\fa \pmb z\in A$.
\end{lemma}
\begin{proof}
$A$ is convex $\implies \overline A$ is convex. Fix points $\pmb x\in\R^{n},\pmb z\in A$. Take the variable point $\pmb z_{t}\sett (1-t)\pi(\pmb x) + t\pmb z$ for $t\in (0,1)$. For brevity we take $s=1-t$. Each $\pmb z_{t}\in \overline A$ as $\overline A$ is convex and both $\pi(\pmb x),\pmb z\in \overline A$. For each $t$ and by construction of $\pi(\pmb x)$, 

\begin{align*}
&\norm{\pmb z_{t}-\pmb x}{}^{2} \ge \norm{\pi(\pmb x)-\pmb x}{}^{2} \\
\implies& \norm{\pmb z_{t}}{}^{2} - 2\pmb x^{\top}\pmb z_{t} \ge \norm{\pi(\pmb x)}{}^{2}-2\pmb x^{\top}\pi(\pmb x)\\
\implies& \norm{\pmb z_{t}}{}^{2} - \norm{\pi(\pmb x)}{}^{2} \ge 2\pmb x^{\top}(\pmb z_{t}-\pi(\pmb x))\\ 
\implies &\left[s^{2}\norm{\pi(\pmb x)}{}^{2} + t^{2}\norm{\pmb z}{}^{2}+ 2st\pmb z^{\top}\pi(\pmb x)\right] - \norm{\pi(\pmb x)}{}^{2}\ge  2t\pmb x^{\top}(\pmb z-\pi(\pmb x))\\
\implies& t^{2}\norm{\pmb z}{}^{2}+ 2st\pmb z^{\top}\pi(\pmb x) + (s-1)(s+1)\norm{\pi(\pmb x)}{}^{2} \ge 2t\pmb x^{\top}(\pmb z-\pi(\pmb x))\\
\implies & t^{2}\norm{\pmb z}{}^{2}+ 2st\pmb z^{\top}\pi(\pmb x) - t(s+1)\norm{\pi(\pmb x)}{}^{2} \ge 2t\pmb x^{\top}(\pmb z-\pi(\pmb x))\\
\implies & t\norm{\pmb z}{}^{2}+ 2(1-t)\pmb z^{\top}\pi(\pmb x) - (2-t)\norm{\pi(\pmb x)}{}^{2} \ge 2\pmb x^{\top}(\pmb z-\pi(\pmb x)) ~~~~~~\left[\because t>0\right]\\
\stackrel{t\to0}{\implies}& 2\pmb z^{\top}\pi(\pmb x) - 2\norm{\pi(\pmb x)}{}^{2} \ge 2\pmb x^{\top}(\pmb z-\pi(\pmb x))\\
\implies & \pi(\pmb x)^{\top}(\pmb z-\pi(\pmb x)) \ge \pmb x^{\top} (\pmb z-\pi(\pmb x))\\
\implies &(\pmb x-\pi(\pmb x))^{\top} (\pmb z-\pi(\pmb x)) \le 0
\end{align*}
\end{proof}

Now assume $A$ is closed so that the argmin actually exists in $A$. Let $\pmb a$ be the given input point. We compute $\ds \pmb y^{*} = \arg \min_{\pmb y\in A} \norm{\pmb y-\pmb a}{2}$ in $\cO(n)$ steps. \\
If $\pmb y^{*} = \pmb a$ then $\pmb a\in A$ and our algorithm returns this. \\
If not, take $\pmb p \sett \frac{\pmb a+\pmb y^{*}}{2}$ and the hyperplane $\cH \sett \set{\pmb x\in \R^{n} \st (\pmb a-\pmb y^{*})^{\top}(\pmb p-\pmb x) = 0}$. This hyperplane passes through $\pmb p$ and is perpendicular to $\pmb a-\pmb y^{*}$. So $(\pmb a-\pmb y^{*})^{\top}(\pmb p - \pmb a) = -(\pmb a-\pmb y^{*})^{\top}\left(\frac{\pmb a-\pmb y^{*}}{2}\right) < 0$ because $\pmb a\ne \pmb y^{*}$. And if $\pmb x\in A$ then $(\pmb a-\pmb y^{*})^{\top}(\pmb p-\pmb x) = (\pmb a-\pmb y^{*})^{\top}(\pmb p-\pmb y^{*}) + (\pmb a-\pmb y^{*})^{\top}(\pmb y^{*}-\pmb x) \stackrel{\text{\Cref{obs}}}{\ge} (\pmb a-\pmb y^{*})^{\top}(\pmb p - \pmb y^{*}) = (\pmb a-\pmb y^{*})^{\top}\left(\frac{\pmb a - \pmb y^{*}}2\right) > 0$. Computing this hyperplane takes $\cO(n)$ steps (for vector multiplication). Since we made only one call to the given blackbox, the overall number of steps is $\cO(n)$.

\item %Note that $E$ can be expressed as $\bigcup\limits_{\pmb a\in A} D_{\epsilon}(\pmb a)$ where $D_{\epsilon}(\pmb a) = \set{\pmb x\in \R^{n}\st \norm{\pmb x-\pmb a}{}\le \epsilon}$. We see this as follows. If $\pmb a\in A$ and $\pmb x\in D_{\epsilon}(\pmb a)$ then clearly $\pmb x\in E$ (because take $\pmb y=\pmb a$ in the description of $E$), so the entire union is contained in $E$. For the other containtment if $\pmb x\in E$ then $\exists \pmb y\in A$ such that $\norm{\pmb y-\pmb x}{}\le \epsilon$ whence $\pmb x\in D_{\epsilon}(\pmb y)$.

We denote $\ds\pi(\pmb x) = \arg\min_{\pmb y\in A} \norm{\pmb y-\pmb x}{}$.

\begin{cl}$E$ is convex. \end{cl}
\begin{proof}Take two points $\pmb x,\pmb y\in E$ and $t\in [0,1]$ and let $s=1-t$. We will show $t\pmb x+s\pmb y\in E$. There are points $a,b\in A$ such that $\norm{\pmb a-\pmb x}{}\le\epsilon, \norm{\pmb b-\pmb y}{}\le \epsilon$. By convexity of $A$, $t\pmb a+s\pmb b\in A$. Clearly $\norm{(t\pmb a+s\pmb b)-(t\pmb x+s\pmb y)}{} = \norm{t(\pmb a-\pmb x) + s(\pmb b-\pmb y)}{} \le t\norm{\pmb a-\pmb x}{} + s \norm{\pmb b-\pmb y}{} \le t\epsilon+s\epsilon=\epsilon$.\end{proof}

\begin{cl}\label{bound}
$\pmb x\in E$ iff $\norm{\pmb x-\pi(\pmb x)}{}\le \epsilon$.\end{cl}
\begin{proof}
Say $\pmb x\in E$. Then $\exists \pmb y\in A$ such that $\norm{\pmb x-\pi(\pmb x)}{} \stackrel{\text{definition of }\pi(\pmb x)}{\le} \norm{\pmb x-\pmb y}{}\le \epsilon$.

Say $\pmb x\notin E$. Then $\forall \pmb y\in A, \norm{\pmb x-\pmb y}{} > \epsilon$. So de definition of $\pi(\pmb x)$, $\norm{\pmb x-\pi(\pmb x)}{} \ge \epsilon$. But since $\pi(\pmb x)\in A$, the equality can't occur as otherwise $\pmb x$ would be in $E$, whence $\norm{\pmb x-\pi(\pmb x)}{} > \epsilon$.
\end{proof}

\begin{cl}\label{projE}
If $\pmb x\notin E$ then $\pi'(\pmb x)\sett \pi(\pmb x) + \epsilon\frac{\pmb x-\pi(\pmb x)}{\norm{\pmb x-\pi(\pmb x)}{}}\in E$ satisfies $\ds\pi'(\pmb x) = \arg\min_{\pmb y\in E}\norm{\pmb x-\pmb y}{}$.
\end{cl}
\begin{proof}
The given point $\pi'(\pmb x)\in E$ because $\norm{\pi'(\pmb x)-\pi(\pmb x)}{}=\epsilon$ and $\pi(\pmb x)\in A$.\\
If $\pmb q\in E$ then $\ds\norm{\pmb x-\pmb q}{} \stackrel{\Delta-\text{ineq}}{\ge} \norm{\pmb x-\pi(\pmb q)}{} - \norm{\pi(\pmb q)-\pmb q}{} ]\stackrel{\text{\Cref{bound}}}{\ge} \norm{\pmb x-\pi(\pmb q)}{} - \epsilon \ge \norm{\pmb x-\pi(\pmb x)}{} - \epsilon = \norm{\pmb x-\pi(\pmb x)}{} \left(1-\frac{\epsilon}{\norm{\pmb x-\pi(\pmb x)}{}}\right) 
= \norm{\pmb x-\pi(\pmb x) - \frac{\epsilon(\pmb x-\pi(\pmb x))}{\norm{\pmb x-\pi(\pmb x)}{}}}{} = \norm{\pmb x-\pi'(\pmb x)}{}$.
\end{proof}


Again assume $A$ is closed. Note that as a byproduct of the proof of \cref{projE}, we showed that $E$ is closed because if $\pmb x\in \partial E$ then $\norm{\pmb x-\pi(\pmb x)}{} = \epsilon$. Our oracle will work as follows for a given $\pmb a$. Since we have an oracle to compute $\pi(\pmb a)$, we can also find $\ds\pi'(\pmb a) = \arg\min_{\pmb y\in E}\norm{\pmb a-\pmb y}{}$ in $\cO(n)$ time by \cref{projE}. This exactly reduces to the previous problem, replacing $A$ by $E$.


\end{enumerate}







\newpage
\pb
In class we designed a $3/4-$approximation for MAX-$2$SAT using LP rounding. The MAX-SAT problem is similar except for the fact that the clauses can contain any number of literals. Formally, the input consists of $n$ boolean variables $x_{1},x_{2},\cdots,x_{n}$ (each may be either $0$ (false) or $1$ (true)), $m$ clauses $C_{1},\cdots,C_{m}$ (each of which consists of disjunction (an “or”) of some number variables or their negations) and a non-negative weight $w_{i}$ for each clause. The objective is to find an assignment of $1$ or $0$ to $x_{i}$'s that maximize the total weight of satisfied clauses. As we saw in the class, a clause is satisfied if one of its non-negated variable is set to $1$, or one of the negated variable is set to $0$. You can assume that no literal is repeated in a clause and at most
one of $x_{i}$ or $\neg x_{i}$ appears in any clause.
\begin{enumerate}[label=(\alph*)]
\item Generalize the LP relaxation for MAX-$2$SAT seen in the class to obtain a LP relaxation of the MAX-SAT problem.
\item Use the standard randomized rounding algorithm (the same one we used in class for MAX-2SAT) on the LP-relaxation you designed in part (1) to give a $(1-1/e)$ approximation algorithm for MAX-SAT. Recall that clauses can be of any length.\\
(Hint: there is a clean way to resolve ``the math'' without excessive calculations).
\item A naive algorithm for MAX-SAT problem is to set each variable to true with probability $1/2$ (without writing any LP). It is easy to see that this unbiased randomized algorithm of MAX-SAT achieves $1/2$-approximation in expectation. Show the algorithm that returns the best of two solutions given by the randomized rounding of the LP and the simple unbiased randomized algorithm is a $3/4-$approximation algorithm of MAX-SAT. (Hint: it may help to realize that in fact randomly selecting one of these two algorithms to run also gives a $3/4-$approximation in expectation).
\item Using the previous part (and in particular, the hint) for intuition, design a direct rounding scheme of your LP relaxation to get a $3/4-$approximation (that is, design a function $f$ which assigns a literal $x_{i}$ to be true independently with probability $f(z)$ when the corresponding variable $z_{i}$ in your LP relaxation is equal to $z$). (Hint: here, it may get messy to fully resolve the calculations. You will get full credit if you state the correct rounding scheme and clearly state the necessary inequalities for the proof. You should also attempt to show that the inequalities hold for your own benefit, but not for full credit).
\end{enumerate}

\soln


\begin{enumerate}[label=(\alph*)]
\item 
MAX-SAT is the following integer program where we have $n$ variables $x_{1},\cdots,x_{n}$ and $m$ clauses (modelled by the values $z_{1},\cdots,z_{m}$) where the $i^{\text{th}}$ clause has $t_{i}$ many literals:
\begin{equation}\begin{aligned}
\max_{\pmb z\in\R^{m}, \pmb x\in \R^{n}} & w_{1}z_{1} + \cdots +w_{m}z_{m}\\
\text{s.t. } & x_{1},\cdots,x_{n} \in \set{0,1}\\
& z_{k}\le y_{k}^{(1)} + \cdots + y_{k}^{(t_{k})} \fa1\le k\le m\\
& z_{k} \le 1 \fa 1\le k\le m
\end{aligned}\end{equation}
where $y_{j}^{(i)}$ are either the literal $x$ corresponding to a variable or their negation $1-x$ counting contribution towards the $j^{\text{th}}$ clause. \textbf{We shall assume that in each clause, the literals come from distinct terms} since repeated literals can be combined into one, and presence of a variable and its negation gives $0$ overall contribution. This integer program can be relaxed to the following LP:
\begin{equation}\label{relax-maxsat}\begin{aligned}
\max_{\pmb z\in\R^{m}, \pmb x\in \R^{n}} & w_{1}z_{1} + \cdots +w_{m}z_{m}\\
\text{s.t. } & x_{1},\cdots,x_{n} \in [0,1]\\
& z_{k}\le y_{k}^{(1)} + \cdots + y_{k}^{(t_{k})} \fa1\le k\le m\\
& z_{k} \le 1 \fa 1\le k\le m.
\end{aligned}\end{equation}
\item Say we solve LP \ref{relax-maxsat} to get the optimal  optimal solution $\overline{z}_{1}, \cdots, \overline{z}_{m}, \overline{x}_{1}, \cdots, \overline{x}_{n}$ with optimal value $\overline Z = \sum\limits_{i=1}^{m}w_{i}\overline z_{i}$. They satisfy $\overline z_{k} \le \min\set{1,\sum\limits_{j=1}^{t_{k}}\overline y_{k}^{(j)}},$ where $\overline y_{k}^{(j)}$ are determined by $\overline x_{i}$'s. \\
(To see why such an optimal solution exists we first observe that adding the constraint $z_{k}\ge 0$ does not change anything because if $z_{k_{0}}<0$ for some $k_{0}$ then force-setting $z_{k_{0}}=0$ only increases the objective. This makes the feasible set compact. We recall the theorem from analysis that any continuous function attains a maxima over a compact set, and conclude the existence of a solution.)

Now we design the following randomized rounding.  For each $1\le i\le n$, take $\tilde x_{i}$ to be $1$ with probability $\overline x_{i}$, and $0$ with probability $1-\overline x_{i}$ (so toss an $\tilde x_{i}-$coin). The $\tilde y_{k}^{(i)}$ are determined by the values of the $\tilde x_{j}$, and so $\tilde z_{k} = \min \set{1, \tilde y_{k}^{(1)} + \cdots + \tilde y_{k}^{(t_{k})}} = \begin{cases}
0 & \text{ if } \tilde y_{k}^{(1)} = \cdots = \tilde y_{k}^{(t_{k})}=0\\
1 & \text{ otherwise}
\end{cases}$ to maximize the total value of the objective. The random variable denoting the objective is $\tilde Z = \sum\limits_{i=1}^{m}w_{i}\tilde z_{i}$ and has expectation $\sum\limits_{i=1}^{m}w_{i}\E{\tilde z_{i}}$. This lets us analyze term-by-term.\\
Let's only focus on one term $\tilde z$ (with the corresponding $\overline z$), say $\tilde z = \tilde z_{1} = \sum\limits_{j=1}^{t_{1}}\tilde y_{1}^{(j)}$. For short we refer to them as only $\tilde y^{(j)}$ and say (without loss of generality) $y^{(j)}$ comes only from the variable $x_{j}$. Further, for $1\le j\le t_{1}$, let $\alpha_{j} \sett \P{\tilde y^{(j)} = 1} = \overline{y}^{(j)} = \begin{cases} \overline x_{p_{j}} & \text{if } y^{(j)} = x_{j}\\
1-\overline{x}_{p_{j}} & \text{if } y^{(j)} = \neg x_{j}
\end{cases}$. Now $\E{\tilde z} = \P{\tilde z=1} = 1-\P{\tilde z=0} = 1-\prod\limits_{j=1}^{t_{1}}\P{y^{(j)}=0} = 1-\prod\limits_{j=1}^{t_{1}}(1-\alpha_{j})$. Denote $S\sett \sum\limits_{j=1}^{t_{1}}\alpha_{j} \ge \overline z$. Combining,
\begin{align*}
\prod_{j=1}^{t_{1}}(1-\alpha_{j}) \stackrel{\text{AM-GM}}{\le} \left(\frac{1}{t_{1}}\sum\limits_{j=1}^{t_{1}}(1-\alpha_{j})\right)^{t_{1}}
= \left(\frac{t_{1}-S}{t_{1}}\right)^{t_{1}} 
= \left(1-\frac{S}{t_{1}}\right)^{t_{1}} \stackrel{S\ge \overline z}{\le} \left(1-\frac{\overline{z}}{t_{1}}\right)^{t_{1}} %\stackrel{1+x\le e^{x}}{\le} e^{-\overline z}
.
\end{align*}

The last inequality is possible because $S=\sum_{j=1}^{t_{1}} \alpha_{j} \le t_{1}\implies 1-\frac{S}{t_{1}} \ge 0$.

Note that $\left.\frac{\d^{2}}{\d z^{2}}\left(1-\frac{\overline{z}}{t_{1}}\right)^{t_{1}}\right|_{z=\overline z} = \frac{t_{1}-1}{t_{1}} \left(1-\frac{\overline{z}}{t_{1}}\right)^{t_{1}-2} \ge 0$ for $t_{1}\ge 1$, hence convex. It follows that $\left(1-\frac{\overline{z}}{t_{1}}\right)^{t_{1}} \le (1-\overline{z})\left(1-\frac{0}{t_{1}}\right)^{t_{1}} + \overline{z} \left(1-\frac{1}{t_{1}}\right)^{t_{1}} \implies 1-\left(1-\frac{\overline{z}}{t_{1}}\right)^{t_{1}} \ge \overline z \left(1-\left(1-\frac{1}{t_{1}}\right)^{t_{1}}\right)$.\\
Thus $\ds\frac{\E{\tilde z}}{\overline z} = \frac{1-\prod\limits_{j=1}^{t_{1}}(1-\alpha_{j})}{\overline z} \ge \frac{1}{\overline{z}}\left(1-\left(1-\frac{\overline z}{t_{1}}\right)^{t_{1}}\right) \ge 1- \left(1-\frac{1}{t_{1}}\right)^{t_{1}} \ge 1-\frac{1}{e}$.

By linearity of expectation, $\E{\tilde Z} = \sum\limits_{i=1}^{m}w_{i}\E{\tilde z_{i}} \ge \sum\limits_{i=1}^{m}\left(1-\frac1e\right)w_{i}\overline z_{i} = \left(1-\frac1e\right)\overline Z$.

\item Let's use the same notation as above, namely $\tilde x_{i},\tilde y_{i},\tilde z_{i}$. We now take $\tilde x_{i}$ to be $0/1$ by tossing a fair coin. Looking at the exact same analysis as above, we have all $\alpha_{i} = \frac12$ which makes $\E{\tilde z_{i}} = 1-2^{-t_{i}}$ which is $\ge \frac12$ because $t_{i}\ge 1$. We conclude by linearity of expectation that this gives a $\frac12-$approximation.

The key point in these two analyses is that for the $LP-$based rounding, we have an approximation ratio $\ge \sum\limits_{i=1}^{m} \left[1- \left(1-\frac{1}{t_{i}}\right)^{t_{i}}\right]$ per clause, where $t_{i}$ is the number of terms in clause $i$, and the unbiased rounding gives a ratio $\ge \sum\limits_{i=1}^{m} 1-\frac{1}{2^{t_{i}}}$ per clause. So if our algorithm was to take the better of the two, namely $\max\set{\tilde Z_{LP}, \tilde Z_{1/2}} \ge \frac{\tilde Z_{LP}+ \tilde Z_{1/2}}2$, where $\tilde Z_{LP},\tilde Z_{1/2}$ are the final values obtained by using the LP-rounding and unbiased rounding respectively, then we get the approximation $\ds\frac{\tilde Z_{LP}+ \tilde Z_{\frac12}}2\ge \sum\limits_{i=1}^{m} \frac{\left(1- \left(1-\frac{1}{t_{i}}\right)^{t_{i}}\right)+\left(1-2^{-t_{i}}\right)}{2}w_{i}\overline z_{i}$.
We note that 
\begin{itemize}[leftmargin=*]
\item $\ds t=1,2\implies \frac{\left(1- \left(1-\frac{1}{t}\right)^{t}\right)+\left(1-2^{-t}\right)}{2} = \frac34$.
\item $\ds t\ge 3\implies \frac{\left(1- \left(1-\frac{1}{t}\right)^{t}\right)+\left(1-2^{-t}\right)}{2} \ge \frac12\left(1-\frac1e+1-\frac18\right) \ge \frac12\left(1-\frac38+1-\frac18\right)=\frac34$ where we used the fact that $e\ge \frac83$.
\end{itemize}
We thus obtained that $\ds\frac{\max\set{\tilde Z_{LP}, \tilde Z_{\frac12}}}{2\overline Z} \ge \frac34$.
\item We are looking for a function $f$ which satisfies that $1-\prod\limits_{i=1}^{t} (1-\alpha_{i}) \ge \frac34 \min\set{1,\sum\limits_{i=1}^{t}\overline y_{i}}$ where $\alpha_{i}$ is $f(\overline{x_{i}})$ or $1-f(\overline x_{i})$ depending on whether the corresponding $y^{(i)}=x_{i}$ or $y^{(i)}=\neg x_{i}$ respectively, and the value of $\overline y_{i} = \neg \overline x_{i}$ is interpreted as $1-\overline x_{i}$ in case $y_{i} = \neg x_{i}$. For simplicity look at a clause with no negated variable and we want to find $f$ such that $1-\prod\limits_{i=1}^{t} (1-f(x_{i})) \ge \frac34 \min\set{1,\sum\limits_{i=1}^{t}x_{i}}$, as long as $f(x) \le 1-f(1-x)$ so that we can replace the corresponding terms in the product. 

{\color{gray} We ideally want to turn product into sum so want to take $f(x) = 1-\alpha^{x}$ so that $\prod(1-f(x_{i})) = \alpha^{\sum x_{i}}$ and can use that to compare with $\frac34\sum x_{i}$. In other words we want to find a constant $\alpha$ such that $1-\alpha^{x} \ge \frac34x$ whenever $x\in[0,1]$. 
}

\begin{lemma}
$4^{-x} \le 1-\min\set{\frac34,\frac{3x}4}\forall x\ge 0$
\end{lemma}
\begin{proof}
If $x\ge 1$ then $4^{-x}\le \frac14 = 1-\frac34 = 1-\min\set{\frac34,\frac{3x}4}$.\\
Assume $x\in [0,1]$. We want to show $4^{-x} \le 1-\frac{3x}4$. The second derivative of $h(x)\sett 1-\frac{3x}{4}-4^{-x}$ is $-\left(\ln(4)\right)^{2}\cdot 4^{-x} <0$ hence concave. Note $h(0)=h(1)=0$ which means $x\in[0,1]\implies h(x) \ge xh(0)+(1-x)h(1) = 0$.
\end{proof}

For a final verification, let's check that $1-4^{-x}\le 4^{x-1}$ for $x\in [0,1]$. Graphing in GeoGebra indicates that $1-4^{-x}- 4^{x-1}\le0$ which can be easily checked with calculus.

So our rounding scheme will be to let $x_{i}$ be $1$ with probability $f(\overline x_{i}) = 1-4^{-\overline x_{i}}$ and $0$ with probability $4^{-\overline x_{i}}$. 
\end{enumerate}





%\newpage
%
%\pb
%Consider the following problem: there are $n>k$ independent (but not
%identically distributed) non-negative random variables $X_{1},\cdots,X_{n}$ drawn according to distributions $D_{1},\cdots,D_{n}$. Initially, you know each $D_{i}$ but none of the $X_{i}$s. Starting from $i= 1$, each $X_{i}$ is revealed one at a time. Immediately after it is revealed, you must decide whether to ``accept $i$'' or ``reject $i$'', before seeing the next $X_{i+1}$. You may accept at most $k$ elements in total (that is, once you’ve accepted $k$ times, you
%must reject everything that comes after). Your reward at the end is $\ds\sum_{i\st i \text{ was accepted}} X_{i}$.
%\begin{enumerate}[label=(\alph*)]
%\item For general $k$, design a policy that guarantees expected reward at least $$\ds\left[1-\cO\left(\sqrt{\frac{\ln k}{k}}\right)\right]\cdot \mathbb{E}_{X_{1},\cdots,X_{n} \leftarrow D_{1},\cdots,D_{n}}\left[\sum_{j=1}^{k}X_{r(j)} \right]$$ where $r$ is a permutation from $[n]$ to $[n]$ satisfying $X_{r(1)} \ge X_{r(2)} \ge \cdots \ge X_{r(n)}$. In other words, the policy gets expected reward at least $1-\cO\left(\sqrt{\frac{\ln k}{k}}\right)$ times the expected sum of top $k$ weights, which is the best you could do even if you knew all the weights up front.
%\item Come up with an example showing that it is not possible to improve the above
%guarantee beyond $(1-\Omega(1/\sqrt k))$.
%\end{enumerate}
%
%\soln
%\begin{enumerate}
%\item
%\item
%\end{enumerate}




\end{document}

